# BeScout - Skalierungsarchitektur

> Ehrliche Analyse: Was funktioniert bis wann, und was muss wann ge√§ndert werden.

---

## Realit√§ts-Check: Wo stehen wir?

```
AKTUELL          ‚Üí  PILOT (v1)      ‚Üí  WACHSTUM (v2)    ‚Üí  SCALE (v3)
Mock-Daten          100-1K User         10K-100K User        1M+ User
useState            Supabase            Supabase + Cache     Custom Backend
Kein Backend        1 DB                DB + Redis           Microservices
Kein Auth           Supabase Auth       Supabase Auth        Custom Auth
Kein Trading        Einfaches Matching  Queue-basiert        Trading Engine
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
WIR SIND HIER ‚óÑ
```

**Die Wahrheit:** Supabase tr√§gt euch bis ~50.000 User. Danach braucht ihr eine andere Architektur. Aber: 99% aller Startups scheitern nicht an der Technik, sondern daran dass sie nie 50K User erreichen. Also: **Richtig bauen f√ºr 50K, vorbereitet sein f√ºr 1M+.**

---

## Die 5 kritischen Systeme und ihre Skalierungsgrenzen

### 1. üè¶ Trading Engine (DPC Marktplatz)

**Das Problem:** Ihr baut im Kern eine B√∂rse. Order Matching, Atomic Transactions, Echtzeit-Preise. Das ist das technisch komplexeste System.

```
PILOT (1K User)
‚îú‚îÄ‚îÄ Supabase PostgreSQL
‚îú‚îÄ‚îÄ Einfaches Matching: Buyer ‚Üî Seller direkt
‚îú‚îÄ‚îÄ DB Transactions f√ºr Atomicity
‚îú‚îÄ‚îÄ Preis-Updates per Polling (3s Interval)
‚îî‚îÄ‚îÄ ‚úÖ Funktioniert problemlos

WACHSTUM (100K User)
‚îú‚îÄ‚îÄ Supabase PostgreSQL + Connection Pooling (PgBouncer)
‚îú‚îÄ‚îÄ Redis f√ºr Orderbook-Cache (Top 50 Bids/Asks)
‚îú‚îÄ‚îÄ Queue (BullMQ) f√ºr Order Matching
‚îú‚îÄ‚îÄ WebSocket f√ºr Echtzeit-Preise
‚îú‚îÄ‚îÄ DB Read Replicas f√ºr Preis-Queries
‚îî‚îÄ‚îÄ ‚ö†Ô∏è Supabase Limits beachten (Realtime: 500 concurrent/Projekt)

SCALE (1M+ User)
‚îú‚îÄ‚îÄ Dedizierte PostgreSQL Cluster (nicht Supabase)
‚îú‚îÄ‚îÄ Redis Cluster f√ºr Orderbook + Session Cache
‚îú‚îÄ‚îÄ Eigene Matching Engine (Rust/Go)
‚îú‚îÄ‚îÄ Event-Driven Architecture (Kafka/NATS)
‚îú‚îÄ‚îÄ Separate Read/Write DBs (CQRS Pattern)
‚îî‚îÄ‚îÄ üî¥ Komplett andere Architektur n√∂tig
```

**Was ihr JETZT richtig machen m√ºsst:**
- Orderbook-Logik als eigenst√§ndigen Service denken (nicht in die Next.js API bauen)
- Alle Trades √ºber eine zentrale `executeTransaction()` Funktion laufen lassen
- Transaktions-Log von Anfang an (jede BSD-Bewegung wird geloggt)
- Preis-Berechnung als eigene Funktion (Floor, VWAP, 24h Change)

### 2. üéÆ Fantasy Scoring Engine

**Das Problem:** Wenn ein Spieltag endet, m√ºssen potenziell 100.000+ Lineups gleichzeitig gescored werden. Das ist ein Batch-Job, kein Request-Response.

```
PILOT (1K User)
‚îú‚îÄ‚îÄ Supabase Edge Function
‚îú‚îÄ‚îÄ Spieltag-Ende ‚Üí Function scored alle Lineups sequentiell
‚îú‚îÄ‚îÄ ~1.000 Lineups √ó 6 Spieler = 6.000 Score-Berechnungen
‚îú‚îÄ‚îÄ Dauer: ~10 Sekunden
‚îî‚îÄ‚îÄ ‚úÖ Kein Problem

WACHSTUM (100K User)
‚îú‚îÄ‚îÄ Dedizierter Worker Service (Railway/Fly.io)
‚îú‚îÄ‚îÄ Queue-basiert: Events werden in Batches verarbeitet
‚îú‚îÄ‚îÄ ~100.000 Lineups = 600.000 Score-Berechnungen
‚îú‚îÄ‚îÄ Parallelisiert: 10 Worker √ó 10.000 Lineups
‚îú‚îÄ‚îÄ Dauer: ~30 Sekunden
‚îî‚îÄ‚îÄ ‚ö†Ô∏è Worker m√ºssen sauber skalieren

SCALE (1M+ User)
‚îú‚îÄ‚îÄ Kubernetes Worker Pool (auto-scaling)
‚îú‚îÄ‚îÄ 1.000.000 Lineups ‚Üí partitioned by Event
‚îú‚îÄ‚îÄ Pre-computed Player Scores (materialized views)
‚îú‚îÄ‚îÄ Eventual Consistency (Scores kommen in Wellen)
‚îú‚îÄ‚îÄ Leaderboard via Redis Sorted Sets
‚îî‚îÄ‚îÄ üî¥ Eigene Infrastruktur n√∂tig
```

**Was ihr JETZT richtig machen m√ºsst:**
- Scoring als reine Funktion: `calculateScore(lineup, playerPerformances) ‚Üí number`
- Kein Side-Effect in der Scoring-Logik (keine DB-Writes drin)
- Player-Performance Daten getrennt von Lineup-Daten speichern
- Event-Ergebnis als eigene Tabelle, nicht inline in der Events-Tabelle

### 3. üì° Echtzeit-Updates (WebSocket/Realtime)

**Das Problem:** Preis-Ticker, Live-Event-Scores, Trading-Updates ‚Äì alles muss in Echtzeit kommen. WebSocket-Connections bei Scale sind teuer.

```
PILOT (1K User)
‚îú‚îÄ‚îÄ Supabase Realtime (inkl. im Plan)
‚îú‚îÄ‚îÄ ~500 gleichzeitige WebSocket-Connections
‚îú‚îÄ‚îÄ Broadcasts f√ºr Preis-Updates
‚îú‚îÄ‚îÄ Presence f√ºr "X User schauen diesen Spieler an"
‚îî‚îÄ‚îÄ ‚úÖ Supabase Pro Plan reicht

WACHSTUM (100K User)
‚îú‚îÄ‚îÄ Supabase Realtime reicht NICHT (Limit: 500-10K concurrent)
‚îú‚îÄ‚îÄ Eigener WebSocket Server (Socket.io / ws)
‚îú‚îÄ‚îÄ Redis Pub/Sub f√ºr Cross-Server Broadcasting
‚îú‚îÄ‚îÄ Channel-basiert: /prices, /events/{id}, /player/{id}
‚îú‚îÄ‚îÄ Nur Subscriptions die der User braucht
‚îî‚îÄ‚îÄ ‚ö†Ô∏è Dedizierter WebSocket Service n√∂tig

SCALE (1M+ User)
‚îú‚îÄ‚îÄ Ably / Pusher / eigener Cluster
‚îú‚îÄ‚îÄ 100K+ gleichzeitige Connections
‚îú‚îÄ‚îÄ Edge-basierte Distribution
‚îú‚îÄ‚îÄ Smart Reconnection + Fallback zu Polling
‚îî‚îÄ‚îÄ üî¥ Managed Service oder eigener Cluster
```

**Was ihr JETZT richtig machen m√ºsst:**
- Realtime-Updates als abstrakten Layer bauen (`useRealtimePrice(playerId)`)
- Dahinter kann Polling, Supabase Realtime, oder WebSocket stecken
- Client muss graceful degraden: WebSocket ‚Üí SSE ‚Üí Polling
- Nicht alles in Echtzeit: Preise ja, Portfolio-Wert kann 30s cached sein

### 4. üíæ Datenbank-Design

**Das Problem:** Schlechtes DB-Design kann man sp√§ter kaum fixen. Millionen Rows mit falschen Indizes = App wird unbenutzbar langsam.

```sql
-- KERN-SCHEMA (von Anfang an richtig designen)

-- Spieler: ~500 Rows (TFF 1. Lig), w√§chst auf ~10.000 (Multi-Liga)
CREATE TABLE players (
  id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
  first_name TEXT NOT NULL,
  last_name TEXT NOT NULL,
  position TEXT NOT NULL CHECK (position IN ('GK','DEF','MID','ATT')),
  club_id UUID REFERENCES clubs(id),
  age INT,
  nationality TEXT,
  contract_end DATE,
  market_value BIGINT DEFAULT 0,
  -- Denormalisierte Stats f√ºr schnelle Queries (werden per Cron aktualisiert)
  stats_matches INT DEFAULT 0,
  stats_goals INT DEFAULT 0,
  stats_assists INT DEFAULT 0,
  perf_l5 NUMERIC(5,2) DEFAULT 0,
  perf_l15 NUMERIC(5,2) DEFAULT 0,
  perf_season NUMERIC(5,2) DEFAULT 0,
  created_at TIMESTAMPTZ DEFAULT NOW(),
  updated_at TIMESTAMPTZ DEFAULT NOW()
);

-- DPC Supply pro Spieler: 1 Row pro Spieler
CREATE TABLE dpc_supply (
  player_id UUID PRIMARY KEY REFERENCES players(id),
  total_float INT NOT NULL DEFAULT 10000,        -- Gesamt DPCs
  club_held INT NOT NULL DEFAULT 5000,            -- Vom Club gehalten
  circulation INT NOT NULL DEFAULT 0,             -- Im Umlauf bei Usern
  on_market INT NOT NULL DEFAULT 0,               -- Aktuell zum Verkauf
  floor_price BIGINT DEFAULT 0,                   -- G√ºnstigstes Angebot
  last_price BIGINT DEFAULT 0,                    -- Letzter Trade-Preis
  price_change_24h NUMERIC(8,4) DEFAULT 0,        -- 24h √Ñnderung %
  volume_24h BIGINT DEFAULT 0,                    -- 24h Handelsvolumen
  updated_at TIMESTAMPTZ DEFAULT NOW()
);
CREATE INDEX idx_dpc_supply_floor ON dpc_supply(floor_price) WHERE on_market > 0;

-- User Holdings: W√§chst schnell! 1M User √ó √ò10 Spieler = 10M Rows
CREATE TABLE holdings (
  id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
  user_id UUID NOT NULL REFERENCES auth.users(id),
  player_id UUID NOT NULL REFERENCES players(id),
  quantity INT NOT NULL DEFAULT 0,
  avg_buy_price BIGINT DEFAULT 0,
  locked_qty INT DEFAULT 0,                       -- In Fantasy Events gesperrt
  locked_event_id UUID,                           -- Welches Event
  created_at TIMESTAMPTZ DEFAULT NOW(),
  updated_at TIMESTAMPTZ DEFAULT NOW(),
  UNIQUE(user_id, player_id)                      -- Ein Eintrag pro User+Player
);
CREATE INDEX idx_holdings_user ON holdings(user_id) WHERE quantity > 0;
CREATE INDEX idx_holdings_player ON holdings(player_id);

-- Orderbook: Aktive Orders. W√§chst und schrumpft. ~100K aktive Orders bei Scale.
CREATE TABLE orders (
  id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
  user_id UUID NOT NULL REFERENCES auth.users(id),
  player_id UUID NOT NULL REFERENCES players(id),
  side TEXT NOT NULL CHECK (side IN ('buy', 'sell')),
  price BIGINT NOT NULL,                          -- Preis in BSD-Cents
  quantity INT NOT NULL,
  filled_qty INT DEFAULT 0,
  status TEXT DEFAULT 'open' CHECK (status IN ('open','partial','filled','cancelled')),
  created_at TIMESTAMPTZ DEFAULT NOW(),
  expires_at TIMESTAMPTZ
);
CREATE INDEX idx_orders_matching ON orders(player_id, side, price, created_at) 
  WHERE status IN ('open', 'partial');

-- Trades: Append-only Log. W√§chst unbegrenzt. Partitioning nach Monat!
CREATE TABLE trades (
  id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
  player_id UUID NOT NULL,
  buyer_id UUID NOT NULL,
  seller_id UUID NOT NULL,
  buy_order_id UUID REFERENCES orders(id),
  sell_order_id UUID REFERENCES orders(id),
  price BIGINT NOT NULL,
  quantity INT NOT NULL,
  pbt_fee BIGINT DEFAULT 0,                      -- 10% ‚Üí PBT Treasury
  platform_fee BIGINT DEFAULT 0,                  -- Platform Cut
  executed_at TIMESTAMPTZ DEFAULT NOW()
) PARTITION BY RANGE (executed_at);

-- Pro Monat eine Partition erstellen:
-- CREATE TABLE trades_2026_01 PARTITION OF trades FOR VALUES FROM ('2026-01-01') TO ('2026-02-01');

-- BSD Wallet: 1 Row pro User
CREATE TABLE wallets (
  user_id UUID PRIMARY KEY REFERENCES auth.users(id),
  balance BIGINT NOT NULL DEFAULT 0,              -- In Cents (1 BSD = 100)
  locked_balance BIGINT DEFAULT 0,                -- In offenen Orders/Events
  total_deposited BIGINT DEFAULT 0,
  total_withdrawn BIGINT DEFAULT 0,
  updated_at TIMESTAMPTZ DEFAULT NOW(),
  CONSTRAINT positive_balance CHECK (balance >= 0)
);

-- BSD Transaktionslog: Append-only. JEDE Bewegung wird geloggt.
CREATE TABLE transactions (
  id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
  user_id UUID NOT NULL,
  type TEXT NOT NULL,                             -- 'trade_buy','trade_sell','event_entry','event_reward','deposit','withdrawal','pbt_payout'
  amount BIGINT NOT NULL,                         -- Positiv = Eingang, Negativ = Ausgang
  balance_after BIGINT NOT NULL,                  -- Saldo danach (f√ºr Audit)
  reference_id UUID,                              -- Trade ID, Event ID, etc.
  description TEXT,
  created_at TIMESTAMPTZ DEFAULT NOW()
) PARTITION BY RANGE (created_at);

-- PBT Treasury: 1 Row pro Spieler
CREATE TABLE pbt_treasury (
  player_id UUID PRIMARY KEY REFERENCES players(id),
  balance BIGINT NOT NULL DEFAULT 0,
  total_inflow BIGINT DEFAULT 0,
  total_distributed BIGINT DEFAULT 0,
  last_distribution_at TIMESTAMPTZ,
  updated_at TIMESTAMPTZ DEFAULT NOW()
);

-- Fantasy Events
CREATE TABLE events (
  id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
  name TEXT NOT NULL,
  type TEXT NOT NULL,                             -- 'bescout','club','sponsor','creator','special'
  status TEXT DEFAULT 'upcoming',                 -- 'upcoming','registering','late-reg','running','scoring','ended'
  format TEXT DEFAULT '6er',
  gameweek INT,
  entry_fee BIGINT DEFAULT 0,
  prize_pool BIGINT DEFAULT 0,
  max_entries INT,
  current_entries INT DEFAULT 0,
  starts_at TIMESTAMPTZ NOT NULL,
  locks_at TIMESTAMPTZ NOT NULL,                  -- Lineup Lock Deadline
  ends_at TIMESTAMPTZ,
  scored_at TIMESTAMPTZ,
  created_by UUID REFERENCES auth.users(id),
  created_at TIMESTAMPTZ DEFAULT NOW()
);
CREATE INDEX idx_events_status ON events(status, starts_at);

-- Lineups: 1M User √ó √ò3 Events/Woche = 3M Lineups/Woche
CREATE TABLE lineups (
  id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
  event_id UUID NOT NULL REFERENCES events(id),
  user_id UUID NOT NULL REFERENCES auth.users(id),
  -- 6er Format: genau 6 Slots
  slot_gk UUID REFERENCES players(id),
  slot_def1 UUID REFERENCES players(id),
  slot_def2 UUID REFERENCES players(id),
  slot_mid1 UUID REFERENCES players(id),
  slot_mid2 UUID REFERENCES players(id),
  slot_att UUID REFERENCES players(id),
  total_score NUMERIC(8,2),
  rank INT,
  reward_amount BIGINT DEFAULT 0,
  submitted_at TIMESTAMPTZ DEFAULT NOW(),
  locked BOOLEAN DEFAULT FALSE,
  UNIQUE(event_id, user_id)                       -- 1 Lineup pro User pro Event
) PARTITION BY RANGE (submitted_at);
CREATE INDEX idx_lineups_event_rank ON lineups(event_id, rank) WHERE rank IS NOT NULL;
```

**Kritische Design-Entscheidungen:**

| Entscheidung | Warum |
|---|---|
| `BIGINT` f√ºr alle Geldbetr√§ge (Cents) | Keine Floating-Point Fehler bei Geld |
| `PARTITION BY RANGE` f√ºr Trades/Transactions/Lineups | Alte Daten archivieren ohne Performance-Verlust |
| Denormalisierte Stats in `players` | Vermeidet JOIN f√ºr die h√§ufigste Query |
| `balance_after` in Transactions | Audit-Trail, Saldo jederzeit nachvollziehbar |
| `UNIQUE(user_id, player_id)` in Holdings | Kein Duplicate-Problem bei Concurrency |
| Separate `dpc_supply` Tabelle | Hot Table (h√§ufig aktualisiert), klein halten |
| `CHECK` Constraints | DB-Level Validierung, nicht nur App-Level |

### 5. üîê Sicherheit & Finanzsystem

**Das Problem:** Ihr verwaltet virtuelles Geld. Jeder Bug = User verliert BSD. Jede Race Condition = Geld wird dupliziert.

**MUSS von Tag 1:**

```
1. ACID Transactions f√ºr ALLE Geldfl√ºsse
   - Kein "erst Geld abziehen, dann DPC zuweisen" als 2 separate Queries
   - ALLES in einer DB Transaction mit SERIALIZABLE Isolation

2. Optimistic Locking f√ºr Orders
   - Version-Counter auf Orders
   - Concurrent Modifications werden abgefangen
   
3. Idempotency Keys
   - Jede Aktion hat eine eindeutige ID
   - Doppelte Requests f√ºhren nicht zu doppelten Trades

4. Rate Limiting
   - Max 10 Orders/Minute pro User
   - Max 100 API Calls/Minute pro User
   - DDoS-Schutz via Cloudflare

5. Audit Log
   - JEDE BSD-Bewegung wird in `transactions` geloggt
   - `balance_after` f√ºr l√ºckenlosen Nachweis
   - Logs sind append-only (kein DELETE erlaubt)

6. Row Level Security (Supabase)
   - User sieht nur eigene Holdings, Orders, Transactions
   - Keine API Route die "alle User Balances" zur√ºckgibt
```

---

## Skalierungs-Fahrplan

### Phase: Pilot (100-1K User) ‚Üê N√ÑCHSTES ZIEL

```
Frontend:  Next.js auf Vercel (Edge Network, global)
Backend:   Supabase Pro ($25/Monat)
           ‚îú‚îÄ‚îÄ PostgreSQL (8GB RAM, 100GB Storage)
           ‚îú‚îÄ‚îÄ Auth (Social + Email)
           ‚îú‚îÄ‚îÄ Realtime (500 concurrent)
           ‚îî‚îÄ‚îÄ Edge Functions (Scoring)
Cache:     Vercel KV (Redis) f√ºr Preis-Cache
Monitoring: Vercel Analytics + Supabase Dashboard
Kosten:    ~$50/Monat
```

### Phase: Wachstum (10K-100K User)

```
Frontend:  Next.js auf Vercel (gleich)
Backend:   Supabase Enterprise ODER Migration
           ‚îú‚îÄ‚îÄ PostgreSQL mit Read Replicas
           ‚îú‚îÄ‚îÄ PgBouncer f√ºr Connection Pooling
           ‚îî‚îÄ‚îÄ Eigene Edge Functions ‚Üí Railway Workers
Cache:     Upstash Redis ($20-200/Monat)
           ‚îú‚îÄ‚îÄ Orderbook Cache (Top Bids/Asks)
           ‚îú‚îÄ‚îÄ Player Price Cache (TTL: 10s)
           ‚îú‚îÄ‚îÄ Session Cache
           ‚îî‚îÄ‚îÄ Rate Limiting
Queue:     BullMQ auf Railway ($20/Monat)
           ‚îú‚îÄ‚îÄ Order Matching Queue
           ‚îú‚îÄ‚îÄ Scoring Queue
           ‚îî‚îÄ‚îÄ Notification Queue
Realtime:  Ably oder Pusher ($50-500/Monat)
Monitoring: Sentry (Errors) + Grafana (Metrics)
Kosten:    ~$300-1.000/Monat
```

### Phase: Scale (1M+ User)

```
Frontend:  Next.js auf Vercel (gleich, Edge-cached)
Backend:   Eigene Services auf Kubernetes (AWS/GCP)
           ‚îú‚îÄ‚îÄ API Gateway (Kong/Express)
           ‚îú‚îÄ‚îÄ Trading Service (Rust/Go) ‚Üê Performance-kritisch
           ‚îú‚îÄ‚îÄ Scoring Service (Node.js Workers)
           ‚îú‚îÄ‚îÄ Notification Service
           ‚îî‚îÄ‚îÄ Admin Service
Database:  
           ‚îú‚îÄ‚îÄ PostgreSQL Cluster (RDS/CloudSQL) - Primary
           ‚îú‚îÄ‚îÄ Read Replicas (2-4 St√ºck)
           ‚îú‚îÄ‚îÄ Redis Cluster (ElastiCache)
           ‚îî‚îÄ‚îÄ ClickHouse f√ºr Analytics
Queue:     Kafka oder NATS
Realtime:  Eigener WebSocket Cluster oder Ably Enterprise
CDN:       Cloudflare (DDoS + Edge Cache)
Monitoring: Datadog oder Grafana Cloud
Kosten:    ~$3.000-10.000/Monat
```

---

## Was ihr JETZT tun m√ºsst (damit Scale sp√§ter m√∂glich ist)

### ‚úÖ Tun (kostet nichts extra, spart euch Monate sp√§ter)

1. **Geld als BIGINT in Cents speichern** ‚Äì Nicht als Float, nicht als Dezimal
2. **Alle DB-Operationen als Transactions** ‚Äì Atomic oder gar nicht
3. **API Layer zwischen Frontend und DB** ‚Äì Frontend ruft nie direkt die DB auf
4. **Abstrakte Data Access Layer** ‚Äì `getPlayerById(id)` statt direkte Supabase Calls in Components
5. **Trades/Transactions partitionieren** ‚Äì Von Tag 1, nicht nachtr√§glich
6. **Idempotency Keys** ‚Äì Jeder Mutating Request hat eine Client-generated UUID
7. **Event-basierte Architektur denken** ‚Äì "Trade executed" als Event, nicht als Seiteneffekt

### ‚ùå NICHT tun (Overengineering f√ºr jetzt)

1. ~~Microservices~~ ‚Üí Monolith ist schneller gebaut und debugged
2. ~~Kubernetes~~ ‚Üí Vercel + Supabase reicht bis 100K
3. ~~Kafka~~ ‚Üí BullMQ reicht als Queue bis 100K
4. ~~Eigene Matching Engine~~ ‚Üí Einfaches DB-basiertes Matching reicht bis 50K
5. ~~Multi-Region~~ ‚Üí Ein Region (eu-central-1) reicht f√ºr T√ºrkei + DACH
6. ~~GraphQL~~ ‚Üí REST/tRPC ist einfacher f√ºr euer Team

### üî∂ Vorbereiten (Interface definieren, Implementation sp√§ter)

```typescript
// Diese Interfaces JETZT definieren, Implementation kommt sp√§ter

interface TradingService {
  placeOrder(order: NewOrder): Promise<Order>;
  cancelOrder(orderId: string): Promise<void>;
  getOrderbook(playerId: string): Promise<Orderbook>;
  executeMatch(buyOrder: Order, sellOrder: Order): Promise<Trade>;
}

interface ScoringService {
  calculatePlayerScore(playerId: string, gameweekId: string): Promise<number>;
  scoreLineup(lineup: Lineup, scores: Map<string, number>): Promise<number>;
  scoreEvent(eventId: string): Promise<EventResult>;
}

interface PriceService {
  getFloorPrice(playerId: string): Promise<number>;
  get24hChange(playerId: string): Promise<number>;
  getPriceHistory(playerId: string, days: number): Promise<PricePoint[]>;
}

interface WalletService {
  getBalance(userId: string): Promise<WalletBalance>;
  credit(userId: string, amount: number, reason: string): Promise<Transaction>;
  debit(userId: string, amount: number, reason: string): Promise<Transaction>;
  transfer(fromId: string, toId: string, amount: number): Promise<Transaction>;
}
```

---

## Konkrete Zahlen: Was h√§lt wie viel aus?

| Komponente | 1K User | 100K User | 1M User |
|---|---|---|---|
| Supabase Free | ‚úÖ | ‚ùå | ‚ùå |
| Supabase Pro ($25) | ‚úÖ | ‚ö†Ô∏è Grenzwertig | ‚ùå |
| Supabase Enterprise | ‚úÖ | ‚úÖ | ‚ö†Ô∏è |
| Vercel Hobby | ‚úÖ | ‚ùå | ‚ùå |
| Vercel Pro ($20) | ‚úÖ | ‚úÖ | ‚ö†Ô∏è |
| Supabase Realtime (500 conn.) | ‚úÖ | ‚ùå | ‚ùå |
| Ably Free (200 conn.) | ‚ö†Ô∏è | ‚ùå | ‚ùå |
| Ably Pro | ‚úÖ | ‚úÖ | ‚úÖ |
| PostgreSQL ohne Replicas | ‚úÖ | ‚ö†Ô∏è | ‚ùå |
| PostgreSQL mit 2 Replicas | ‚úÖ | ‚úÖ | ‚ö†Ô∏è |
| Redis (single node) | ‚úÖ | ‚úÖ | ‚ö†Ô∏è |

---

## TL;DR

**Jetzt:** Supabase + Vercel bauen, aber die Interfaces sauber halten.
**Bei 10K Usern:** Redis dazuschalten, Scoring als eigenen Worker auslagern.
**Bei 100K Usern:** Von Supabase Realtime auf Ably/Pusher wechseln, Read Replicas.
**Bei 1M Usern:** Trading Engine in Rust/Go, eigene Infra, Kubernetes.

Die Frontend-Architektur (Next.js + React) skaliert problemlos. Die Skalierungsprobleme kommen ALLE vom Backend. Deswegen ist es so wichtig, dass die **Interfaces** jetzt sauber sind ‚Äì die Implementation kann man sp√§ter tauschen.
